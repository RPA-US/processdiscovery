{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLIP Process Discovery Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import gc\n",
    "import random\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from ipywidgets import GridBox, Layout\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import imagehash\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# TensorFlow related imports\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "# PM4Py related imports\n",
    "# import pm4py\n",
    "# from pm4py.objects.conversion.log import converter as log_converter\n",
    "# from pm4py.algo.discovery.inductive import algorithm as inductive_miner\n",
    "# from pm4py.objects.conversion.process_tree import converter as pt_converter\n",
    "# from pm4py.visualization.petri_net import visualizer as pn_visualizer\n",
    "# from pm4py.visualization.bpmn import visualizer as bpmn_visualizer\n",
    "# from pm4py.objects.bpmn.exporter import exporter as bpmn_exporter\n",
    "# from pm4py.algo.evaluation.replay_fitness import algorithm as replay_fitness_evaluator\n",
    "# from pm4py.algo.evaluation.precision import algorithm as precision_evaluator\n",
    "# from pm4py.algo.evaluation.generalization import algorithm as generalization_evaluator\n",
    "# from pm4py.algo.evaluation.simplicity import algorithm as simplicity_evaluator\n",
    "# from transformers import CLIPModel, CLIPProcessor, CLIPTokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lectura fichero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ui_log_as_dataframe(log_path):\n",
    "  return pd.read_csv(log_path, sep=\";\")#, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracción de características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_from_images(df, image_col, text_col, image_weight, text_weight, img_dir):\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "    combined_features = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        text = row[text_col]\n",
    "        # Usa os.path.join para construir la ruta completa de la imagen.\n",
    "        image_path = os.path.join(img_dir, row[image_col])\n",
    "        \n",
    "        # Asegúrate de que la imagen exista, de lo contrario lanza un error.\n",
    "        if not os.path.exists(image_path):\n",
    "            raise ValueError(f\"La imagen no existe en {image_path}\")\n",
    "\n",
    "        # Abre la imagen usando la ruta completa.\n",
    "        image = Image.open(image_path)\n",
    "        inputs = processor(text=[text], images=image, return_tensors=\"pt\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        image_features = outputs.image_embeds.cpu().numpy().flatten() * image_weight\n",
    "        text_features = outputs.text_embeds.cpu().numpy().flatten() * text_weight\n",
    "        \n",
    "        combined_feature = np.hstack((image_features, text_features))\n",
    "        combined_features.append(combined_feature)\n",
    "\n",
    "    df['combined_features'] = combined_features\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_from_images_with_tokenizer(df, image_col, text_col, image_weight, text_weight, img_dir, header_txt=False, text_path_col=\"header_txt\"):\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "    combined_features = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        if header_txt:\n",
    "            txt_path = os.path.join(\"logs/invoice_def\", \"ocr_results\", row[text_path_col])\n",
    "            if not os.path.exists(txt_path):\n",
    "                raise FileNotFoundError(f\"El archivo de texto no existe: {txt_path}\")\n",
    "            with open(txt_path, 'r') as file:\n",
    "                text = file.read()\n",
    "        else:\n",
    "            text = row[text_col]\n",
    "\n",
    "        # Tokenizar el texto\n",
    "        input_ids = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "        \n",
    "        # Construye la ruta completa de la imagen y asegura que exista.\n",
    "        image_path = os.path.join(img_dir, row[image_col])\n",
    "        if not os.path.exists(image_path):\n",
    "            raise ValueError(f\"La imagen no existe en {image_path}\")\n",
    "\n",
    "        # Abre la imagen y la procesa con el modelo CLIP\n",
    "        image = Image.open(image_path)\n",
    "        image_inputs = processor(images=[image], return_tensors=\"pt\")\n",
    "\n",
    "        # Combina las entradas de texto e imagen y pasarlo al modelo\n",
    "        inputs = {'input_ids': input_ids['input_ids'], 'attention_mask': input_ids['attention_mask'], 'pixel_values': image_inputs['pixel_values']}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        image_features = outputs.image_embeds.cpu().numpy().flatten() * image_weight\n",
    "        text_features = outputs.text_embeds.cpu().numpy().flatten() * text_weight\n",
    "        combined_feature = np.hstack((image_features, text_features))\n",
    "        combined_features.append(combined_feature)\n",
    "\n",
    "    df['combined_features'] = combined_features\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clusterización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_images(df, n_clusters_range, use_pca, n_components):\n",
    "    features = np.array(df['combined_features'].tolist())\n",
    "    \n",
    "    if use_pca:\n",
    "        pca = PCA(n_components=n_components)\n",
    "        features = pca.fit_transform(features)\n",
    "        print(f\"PCA aplicado: {features.shape[1]} componentes retenidos\")\n",
    "\n",
    "    clustering_scores = {\n",
    "        'n_clusters': [],\n",
    "        'silhouette_score': [],\n",
    "        'davies_bouldin_score': [],\n",
    "        'calinski_harabasz_score': []\n",
    "    }\n",
    "\n",
    "    for k in range(*n_clusters_range):\n",
    "        clustering = AgglomerativeClustering(n_clusters=k).fit(features)\n",
    "        labels = clustering.labels_\n",
    "\n",
    "        clustering_scores['n_clusters'].append(k)\n",
    "        clustering_scores['silhouette_score'].append(silhouette_score(features, labels))\n",
    "        clustering_scores['davies_bouldin_score'].append(davies_bouldin_score(features, labels))\n",
    "        clustering_scores['calinski_harabasz_score'].append(calinski_harabasz_score(features, labels))\n",
    "\n",
    "    # Encuentra el índice del número óptimo de clústeres basado en la mejor puntuación Silhouette\n",
    "    optimal_index = np.argmax(clustering_scores['silhouette_score'])\n",
    "    optimal_clusters = clustering_scores['n_clusters'][optimal_index]\n",
    "\n",
    "    # Ejecutar el clustering con el número óptimo de clústeres\n",
    "    best_clustering = AgglomerativeClustering(n_clusters=optimal_clusters).fit(features)\n",
    "    df['activity_label'] = best_clustering.labels_\n",
    "\n",
    "    # Obtener las métricas para el número óptimo de clústeres\n",
    "    optimal_metrics = {\n",
    "        'silhouette_score': clustering_scores['silhouette_score'][optimal_index],\n",
    "        'davies_bouldin_score': clustering_scores['davies_bouldin_score'][optimal_index],\n",
    "        'calinski_harabasz_score': clustering_scores['calinski_harabasz_score'][optimal_index]\n",
    "    }\n",
    "\n",
    "    return df, clustering_scores, optimal_clusters, optimal_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminar_acciones_duplicadas(df, columna_label='activity_label'):\n",
    "    while True:\n",
    "        mascaras_para_eliminar = df[columna_label].eq(df[columna_label].shift())\n",
    "        if mascaras_para_eliminar.sum() == 0:\n",
    "            break\n",
    "        df = df[~mascaras_para_eliminar].reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#invoice def (+1 path 'customer path')\n",
    "log_path = 'logs/invoice_def/log.csv'\n",
    "image_col = 'screenshot'\n",
    "image_dir = 'resources/invoice_def'\n",
    "text_col = 'header'\n",
    "timestamp_col = 'timestamp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lanzar / Guardar ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraciones iniciales\n",
    "model = 'clip'\n",
    "n_clusters_range = (2, 11)\n",
    "n_components = 0.95\n",
    "use_pca = False\n",
    "tokeniza = False #¿Tokenizamos?\n",
    "header_txt = False #¿Usamos el texto completo?\n",
    "\n",
    "# Directorio principal para los casos de estudio\n",
    "ground_truth_colname='ground_truth'\n",
    "case_study_name = \"sc50_rebuild_text_token\" \n",
    "root_dir = os.path.join(\"executions\", case_study_name)\n",
    "os.makedirs(root_dir, exist_ok=True)\n",
    "\n",
    "results = []\n",
    "\n",
    "# Información de las ejecuciones a realizar\n",
    "executions = [\n",
    "    {'exec': 1, 'image_weight': 1, 'text_weight': 0},\n",
    "    {'exec': 2, 'image_weight': 0.8, 'text_weight': 0.2},\n",
    "    {'exec': 3, 'image_weight': 0.6, 'text_weight': 0.4},\n",
    "    {'exec': 4, 'image_weight': 0.5, 'text_weight': 0.5},\n",
    "    {'exec': 5, 'image_weight': 0.4, 'text_weight': 0.6},\n",
    "    {'exec': 6, 'image_weight': 0.2, 'text_weight': 0.8},\n",
    "    {'exec': 7, 'image_weight': 0, 'text_weight': 1}\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3819150546.py, line 24)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[44], line 24\u001b[1;36m\u001b[0m\n\u001b[1;33m    def accuracy_calculation(df, activity_label, ground_truth_colname=def overwrite_csv(df, file_path):\u001b[0m\n\u001b[1;37m                                                                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def overwrite_csv(df, file_path):\n",
    "    \"\"\"Escribe un DataFrame a un archivo CSV, sobrescribiendo el archivo existente.\"\"\"\n",
    "    try:\n",
    "        if os.path.exists(file_path):\n",
    "            os.remove(file_path)\n",
    "        df.to_csv(file_path, index=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Error al escribir el archivo CSV: {e}\")\n",
    "\n",
    "def move_and_overwrite(source, destination):\n",
    "    \"\"\"Mueve un archivo de una ubicación a otra y lo sobrescribe si ya existe.\"\"\"\n",
    "    if os.path.exists(destination):\n",
    "        os.remove(destination)\n",
    "    shutil.move(source, destination)\n",
    "    \n",
    "def clear_caches():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "def load_fresh_data():\n",
    "    return read_ui_log_as_dataframe(log_path)\n",
    "\n",
    "def accuracy_calculation(df, activity_label, ground_truth_colname='ground_truth'):\n",
    "    # Obtener las listas únicas de clústeres predichos y ground_truth\n",
    "    predicted_clusters = df[activity_label].unique()\n",
    "    true_clusters = df[ground_truth_colname].unique()\n",
    "\n",
    "    # Crear la matriz de costos\n",
    "    cost_matrix = np.zeros((len(predicted_clusters), len(true_clusters)))\n",
    "\n",
    "    for i, pred_cluster in enumerate(predicted_clusters):\n",
    "        for j, true_cluster in enumerate(true_clusters):\n",
    "            # Calcular el número de veces que un clúster predicho se asocia a un clúster verdadero\n",
    "            cost_matrix[i, j] = -((df[activity_label] == pred_cluster) & (df[ground_truth_colname] == true_cluster)).sum()\n",
    "\n",
    "    # Usar el algoritmo de asignación húngaro para minimizar el costo\n",
    "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "\n",
    "    # Crear un diccionario de mapeo de clústeres predichos a ground truth\n",
    "    cluster_mapping = {predicted_clusters[row]: true_clusters[col] for row, col in zip(row_ind, col_ind)}\n",
    "\n",
    "    # Mapear los clústeres predichos a los clústeres ground truth\n",
    "    df['mapped_prediction'] = df[activity_label].map(cluster_mapping)\n",
    "\n",
    "    # Evaluar la precisión (accuracy)\n",
    "    accuracy = accuracy_score(df[ground_truth_colname], df['mapped_prediction'])\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "    # Evaluar la precisión (precision) y el recall\n",
    "    precision = precision_score(df[ground_truth_colname], df['mapped_prediction'], average='macro')\n",
    "    recall = recall_score(df[ground_truth_colname], df['mapped_prediction'], average='macro')\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    \n",
    "    return accuracy, recall, precision\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anton\\docs\\01. Tesis\\processdiscovery\\env\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:290: RuntimeWarning: invalid value encountered in cast\n",
      "  return x.astype(dtype, copy=copy, casting=casting)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input y_pred contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 26\u001b[0m\n\u001b[0;32m     22\u001b[0m     df \u001b[38;5;241m=\u001b[39m extract_features_from_images(df, image_col, text_col, image_weight, text_weight, image_dir)\n\u001b[0;32m     24\u001b[0m df, clustering_scores, optimal_clusters, optimal_metrics \u001b[38;5;241m=\u001b[39m cluster_images(df, n_clusters_range, use_pca, n_components)\n\u001b[1;32m---> 26\u001b[0m accuracy, precision, recall \u001b[38;5;241m=\u001b[39m \u001b[43maccuracy_calculation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mactivity_label\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mground_truth_colname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(exec_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdf.csv\u001b[39m\u001b[38;5;124m'\u001b[39m), index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     31\u001b[0m results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexec\u001b[39m\u001b[38;5;124m'\u001b[39m: exec[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexec\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_weight\u001b[39m\u001b[38;5;124m'\u001b[39m: image_weight,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m: accuracy\n\u001b[0;32m     41\u001b[0m })\n",
      "Cell \u001b[1;32mIn[43], line 47\u001b[0m, in \u001b[0;36maccuracy_calculation\u001b[1;34m(df, activity_label, ground_truth_colname)\u001b[0m\n\u001b[0;32m     44\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmapped_prediction\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[activity_label]\u001b[38;5;241m.\u001b[39mmap(cluster_mapping)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Evaluar la precisión (accuracy)\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43maccuracy_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mground_truth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmapped_prediction\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Evaluar la precisión (precision) y el recall\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\anton\\docs\\01. Tesis\\processdiscovery\\env\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\anton\\docs\\01. Tesis\\processdiscovery\\env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:213\u001b[0m, in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \n\u001b[0;32m    149\u001b[0m \u001b[38;5;124;03mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;124;03m0.5\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[1;32m--> 213\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\anton\\docs\\01. Tesis\\processdiscovery\\env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:87\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     85\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[0;32m     86\u001b[0m type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 87\u001b[0m type_pred \u001b[38;5;241m=\u001b[39m \u001b[43mtype_of_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my_pred\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m y_type \u001b[38;5;241m=\u001b[39m {type_true, type_pred}\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;241m==\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n",
      "File \u001b[1;32mc:\\Users\\anton\\docs\\01. Tesis\\processdiscovery\\env\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:389\u001b[0m, in \u001b[0;36mtype_of_target\u001b[1;34m(y, input_name)\u001b[0m\n\u001b[0;32m    387\u001b[0m     data \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;28;01mif\u001b[39;00m issparse(y) \u001b[38;5;28;01melse\u001b[39;00m y\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m xp\u001b[38;5;241m.\u001b[39many(data \u001b[38;5;241m!=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(data, \u001b[38;5;28mint\u001b[39m)):\n\u001b[1;32m--> 389\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    390\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontinuous\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m suffix\n\u001b[0;32m    392\u001b[0m \u001b[38;5;66;03m# Check multiclass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\anton\\docs\\01. Tesis\\processdiscovery\\env\\Lib\\site-packages\\sklearn\\utils\\validation.py:126\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 126\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\anton\\docs\\01. Tesis\\processdiscovery\\env\\Lib\\site-packages\\sklearn\\utils\\validation.py:175\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    161\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    163\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    174\u001b[0m     )\n\u001b[1;32m--> 175\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input y_pred contains NaN."
     ]
    }
   ],
   "source": [
    "for exec in executions:\n",
    "\n",
    "    df = read_ui_log_as_dataframe(log_path)\n",
    "    clear_caches()  \n",
    "\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(42)\n",
    "        \n",
    "    exec_dir = f\"{case_study_name}_{exec['image_weight']}_{exec['text_weight']}\"\n",
    "    exec_path = os.path.join(root_dir, exec_dir)\n",
    "    os.makedirs(exec_path, exist_ok=True)\n",
    "\n",
    "    image_weight = exec['image_weight']\n",
    "    text_weight = exec['text_weight']\n",
    "    \n",
    "    if tokeniza:\n",
    "        df = extract_features_from_images_with_tokenizer(df, image_col, text_col, image_weight, text_weight, image_dir, header_txt, text_path_col='header_txt')\n",
    "    else:\n",
    "        df = extract_features_from_images(df, image_col, text_col, image_weight, text_weight, image_dir)\n",
    "    \n",
    "    df, clustering_scores, optimal_clusters, optimal_metrics = cluster_images(df, n_clusters_range, use_pca, n_components)\n",
    "\n",
    "    accuracy, precision, recall = accuracy_calculation(df, 'activity_label', ground_truth_colname)\n",
    "\n",
    "    df.to_csv(os.path.join(exec_path, 'df.csv'), index=False)\n",
    "\n",
    "\n",
    "    results.append({\n",
    "        'exec': exec['exec'],\n",
    "        'image_weight': image_weight,\n",
    "        'text_weight': text_weight,\n",
    "        'Silhouette': optimal_metrics['silhouette_score'],\n",
    "        'Davies-Bouldin': optimal_metrics['davies_bouldin_score'],\n",
    "        'Calinski-Harabasz': optimal_metrics['calinski_harabasz_score'],\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'Accuracy': accuracy\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "overwrite_csv(results_df, os.path.join(root_dir, 'resultados.csv'))\n",
    "\n",
    "df.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
