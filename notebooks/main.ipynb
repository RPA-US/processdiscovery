{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLIP Process Discovery Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import imagehash\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import GridBox, Layout\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "# TensorFlow related imports\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# PM4Py related imports\n",
    "import pm4py\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "from pm4py.algo.discovery.inductive import algorithm as inductive_miner\n",
    "from pm4py.objects.conversion.process_tree import converter as pt_converter\n",
    "from pm4py.visualization.petri_net import visualizer as pn_visualizer\n",
    "from pm4py.visualization.bpmn import visualizer as bpmn_visualizer\n",
    "from pm4py.objects.bpmn.exporter import exporter as bpmn_exporter\n",
    "from pm4py.algo.evaluation.replay_fitness import algorithm as replay_fitness_evaluator\n",
    "from pm4py.algo.evaluation.precision import algorithm as precision_evaluator\n",
    "from pm4py.algo.evaluation.generalization import algorithm as generalization_evaluator\n",
    "from pm4py.algo.evaluation.simplicity import algorithm as simplicity_evaluator\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import CLIPModel, CLIPProcessor, CLIPTokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lectura fichero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ui_log_as_dataframe(log_path):\n",
    "  return pd.read_csv(log_path, sep=\";\")#, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracción de características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(tokenizer, text):\n",
    "    \"\"\"Tokeniza el texto y devuelve los tokens y su longitud.\"\"\"\n",
    "    tokens = tokenizer.encode(text, return_tensors=\"pt\", truncation=True)\n",
    "    token_length = len(tokens[0])\n",
    "    decoded_text = tokenizer.decode(tokens[0], skip_special_tokens=True)\n",
    "    return tokens, token_length, decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_from_images(df, image_col, text_col, image_weight, text_weight, img_dir):\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "    combined_features = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        text = row[text_col]\n",
    "        # Usa os.path.join para construir la ruta completa de la imagen.\n",
    "        image_path = os.path.join(img_dir, row[image_col])\n",
    "        \n",
    "        # Asegúrate de que la imagen exista, de lo contrario lanza un error.\n",
    "        if not os.path.exists(image_path):\n",
    "            raise ValueError(f\"La imagen no existe en {image_path}\")\n",
    "\n",
    "        # Abre la imagen usando la ruta completa.\n",
    "        image = Image.open(image_path)\n",
    "        inputs = processor(text=[text], images=image, return_tensors=\"pt\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        image_features = outputs.image_embeds.cpu().numpy().flatten() * image_weight\n",
    "        text_features = outputs.text_embeds.cpu().numpy().flatten() * text_weight\n",
    "        \n",
    "        combined_feature = np.hstack((image_features, text_features))\n",
    "        combined_features.append(combined_feature)\n",
    "\n",
    "    df['combined_features'] = combined_features\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clusterización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_images(df, n_clusters_range, use_pca, n_components):\n",
    "    features = np.array(df['combined_features'].tolist())\n",
    "    \n",
    "    if use_pca:\n",
    "        pca = PCA(n_components=n_components)\n",
    "        features = pca.fit_transform(features)\n",
    "        print(f\"PCA aplicado: {features.shape[1]} componentes retenidos\")\n",
    "\n",
    "    clustering_scores = {\n",
    "        'n_clusters': [],\n",
    "        'silhouette_score': [],\n",
    "        'davies_bouldin_score': [],\n",
    "        'calinski_harabasz_score': []\n",
    "    }\n",
    "\n",
    "    for k in range(*n_clusters_range):\n",
    "        clustering = AgglomerativeClustering(n_clusters=k).fit(features)\n",
    "        labels = clustering.labels_\n",
    "\n",
    "        clustering_scores['n_clusters'].append(k)\n",
    "        clustering_scores['silhouette_score'].append(silhouette_score(features, labels))\n",
    "        clustering_scores['davies_bouldin_score'].append(davies_bouldin_score(features, labels))\n",
    "        clustering_scores['calinski_harabasz_score'].append(calinski_harabasz_score(features, labels))\n",
    "\n",
    "    # Encuentra el índice del número óptimo de clústeres basado en la mejor puntuación Silhouette\n",
    "    optimal_index = np.argmax(clustering_scores['silhouette_score'])\n",
    "    optimal_clusters = clustering_scores['n_clusters'][optimal_index]\n",
    "\n",
    "    # Ejecutar el clustering con el número óptimo de clústeres\n",
    "    best_clustering = AgglomerativeClustering(n_clusters=optimal_clusters).fit(features)\n",
    "    df['activity_label'] = best_clustering.labels_\n",
    "\n",
    "    # Obtener las métricas para el número óptimo de clústeres\n",
    "    optimal_metrics = {\n",
    "        'silhouette_score': clustering_scores['silhouette_score'][optimal_index],\n",
    "        'davies_bouldin_score': clustering_scores['davies_bouldin_score'][optimal_index],\n",
    "        'calinski_harabasz_score': clustering_scores['calinski_harabasz_score'][optimal_index]\n",
    "    }\n",
    "\n",
    "    return df, clustering_scores, optimal_clusters, optimal_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clustering_scores(clustering_scores):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    # Silhouette Score\n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.plot(clustering_scores['n_clusters'], clustering_scores['silhouette_score'], marker='o')\n",
    "    plt.title('Silhouette Score')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Score')\n",
    "\n",
    "    # Davies-Bouldin Score\n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.plot(clustering_scores['n_clusters'], clustering_scores['davies_bouldin_score'], marker='o', color='red')\n",
    "    plt.title('Davies-Bouldin Score')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Score')\n",
    "\n",
    "    # Calinski-Harabasz Score\n",
    "    plt.subplot(3, 1, 3)\n",
    "    plt.title('Calinski-Harabasz Score')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Score')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/clustering_scores.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_caminos(df):\n",
    "    caminos = df.groupby('process_id')['activity_label'].apply(tuple)\n",
    "    return caminos\n",
    "\n",
    "def calcular_metricas(caminos_logs, caminos_apriori, caminos_inicial, caminos_final):\n",
    "    caminos_logs_set = set(caminos_logs)\n",
    "    caminos_apriori_set = set(caminos_apriori)\n",
    "    caminos_inicial_set = set(caminos_inicial)\n",
    "    caminos_final_set = set(caminos_final)\n",
    "    \n",
    "    # Calcular las métricas\n",
    "    num_paths_apriori = len(caminos_apriori_set)\n",
    "    num_paths_inicial = len(caminos_inicial_set)\n",
    "    num_paths_final = len(caminos_final_set)\n",
    "    \n",
    "    # Porcentajes de nuevos caminos y caminos no descubiertos\n",
    "    new_paths = caminos_final_set - caminos_apriori_set\n",
    "    percent_new = len(new_paths) / num_paths_final if num_paths_final else 0\n",
    "    \n",
    "    non_discovered_paths = caminos_apriori_set - caminos_final_set\n",
    "    percent_non_discovered = len(non_discovered_paths) / num_paths_apriori if num_paths_apriori else 0\n",
    "    \n",
    "    return {\n",
    "        'num_paths_apriori': num_paths_apriori,\n",
    "        'num_paths_inicial': num_paths_inicial,\n",
    "        'num_paths_final': num_paths_final,\n",
    "        'percent_new': percent_new * 100,\n",
    "        'percent_non_discovered': percent_non_discovered * 100\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case id allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_process_id_assignment(df):\n",
    "    activity_inicial = df['activity_label'].iloc[0]\n",
    "    process_id = 1\n",
    "    process_ids = [process_id]  \n",
    "    for index, row in df.iterrows():\n",
    "        if index != 0:  \n",
    "            if row['activity_label'] == activity_inicial:\n",
    "                process_id += 1\n",
    "            process_ids.append(process_id)\n",
    "        else:\n",
    "            continue\n",
    "    df['process_id'] = process_ids\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminar_acciones_duplicadas(df, columna_label='activity_label'):\n",
    "    mascaras_para_eliminar = df[columna_label].eq(df[columna_label].shift())\n",
    "    df_limpio = df[~mascaras_para_eliminar]\n",
    "    \n",
    "    return df_limpio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bpmn / Petrinet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def petri_net_process(df, timestamp_col):\n",
    "    # DataFrame To EventLog\n",
    "    formatted_df = pm4py.format_dataframe(df, case_id='process_id', activity_key='activity_label', timestamp_key=timestamp_col)\n",
    "    event_log = pm4py.convert_to_event_log(formatted_df)\n",
    "\n",
    "    # Descubrimiento del árbol del proceso\n",
    "    process_tree = inductive_miner.apply(event_log)\n",
    "    net, initial_marking, final_marking = pm4py.convert_to_petri_net(process_tree)\n",
    "\n",
    "    # Métricas\n",
    "    fitness = replay_fitness_evaluator.apply(event_log, net, initial_marking, final_marking)\n",
    "    precision = precision_evaluator.apply(event_log, net, initial_marking, final_marking)\n",
    "    generalization = generalization_evaluator.apply(event_log, net, initial_marking, final_marking)\n",
    "    simplicity = simplicity_evaluator.apply(net)\n",
    "\n",
    "    # Guardar resultados\n",
    "    dot = pn_visualizer.apply(net, initial_marking, final_marking)\n",
    "    dot_path = os.path.join('results', 'pn.dot')\n",
    "    with open(dot_path, 'w') as f:\n",
    "        f.write(dot.source)\n",
    "\n",
    "    return fitness, precision, generalization, simplicity\n",
    "\n",
    "def bpmn_process(df, timestamp_col):\n",
    "    # DataFrame To EventLog\n",
    "    formatted_df = pm4py.format_dataframe(df, case_id='process_id', activity_key='activity_label', timestamp_key=timestamp_col)\n",
    "    event_log = pm4py.convert_to_event_log(formatted_df)\n",
    "\n",
    "    # Descubrimiento del modelo BPMN\n",
    "    bpmn_model = pm4py.discover_bpmn_inductive(event_log)\n",
    "\n",
    "    # Guardar resultados\n",
    "    dot = bpmn_visualizer.apply(bpmn_model)\n",
    "    dot_path = os.path.join('results', 'bpmn.dot')\n",
    "    with open(dot_path, 'w') as f:\n",
    "        f.write(dot.source)\n",
    "    bpmn_exporter.apply(bpmn_model, os.path.join('results', 'bpmn.bpmn'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#invoice def (+1 path 'customer path')\n",
    "log_path = 'logs/invoice_def/log.csv'\n",
    "image_col = 'screenshot'\n",
    "image_dir = 'resources/invoice_def'\n",
    "text_col = 'header'\n",
    "timestamp_col = 'timestamp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lanzar / Guardar ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#invoice customer path\n",
    "caminos_apriori = ((7, 4, 5, 3, 1, 0), (7, 4, 6, 2))\n",
    "caminos_apriori_series = pd.Series(list(caminos_apriori))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraciones iniciales\n",
    "model = 'clip'\n",
    "n_clusters_range = (2, 11)\n",
    "n_components = 0.95\n",
    "use_pca = False\n",
    "header_txt_file = False\n",
    "\n",
    "# Directorio principal para los casos de estudio\n",
    "case_study_name = \"invoice_def_header\" \n",
    "root_dir = os.path.join(\"executions\", case_study_name)\n",
    "os.makedirs(root_dir, exist_ok=True)\n",
    "\n",
    "results = []\n",
    "\n",
    "# Información de las ejecuciones a realizar\n",
    "executions = [\n",
    "    {'exec': 1, 'image_weight': 1, 'text_weight': 0},\n",
    "    {'exec': 2, 'image_weight': 0.8, 'text_weight': 0.2},\n",
    "    {'exec': 3, 'image_weight': 0.6, 'text_weight': 0.4},\n",
    "    {'exec': 4, 'image_weight': 0.4, 'text_weight': 0.6},\n",
    "    {'exec': 5, 'image_weight': 0.2, 'text_weight': 0.8},\n",
    "    {'exec': 6, 'image_weight': 0, 'text_weight': 1},\n",
    "    {'exec': 7, 'image_weight': 0.5, 'text_weight': 0.5}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overwrite_csv(df, file_path):\n",
    "    \"\"\"Escribe un DataFrame a un archivo CSV, sobrescribiendo el archivo existente.\"\"\"\n",
    "    try:\n",
    "        if os.path.exists(file_path):\n",
    "            os.remove(file_path)\n",
    "        df.to_csv(file_path, index=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Error al escribir el archivo CSV: {e}\")\n",
    "\n",
    "def move_and_overwrite(source, destination):\n",
    "    \"\"\"Mueve un archivo de una ubicación a otra y lo sobrescribe si ya existe.\"\"\"\n",
    "    if os.path.exists(destination):\n",
    "        os.remove(destination)\n",
    "    shutil.move(source, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3fdd1451dc34ce3ae11246118045234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "aligning log, completed variants ::   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbd34e4471d74af48c787ad2e2f12fa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "computing precision with alignments, completed variants ::   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f52dc04e5f3f4d7aabdd91014603ccdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "replaying log with TBR, completed traces ::   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "889b9b9cd5c740dfbd22ca88e1ed7977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "aligning log, completed variants ::   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d558deee49844e028f52919c891dbddd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "computing precision with alignments, completed variants ::   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea82b66598a141a4bbabb69f2561e0a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "replaying log with TBR, completed traces ::   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e31f1a41e5f34022ac2db690c7300a5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "aligning log, completed variants ::   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0868b0bc835145a9b584d01b90f41e66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "computing precision with alignments, completed variants ::   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5db2027fe6954232ad3e76b3186081d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "replaying log with TBR, completed traces ::   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c84a3f07d744cc68ed536112612bdc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "aligning log, completed variants ::   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3193090a420f4a6c8c611138a645dd5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "computing precision with alignments, completed variants ::   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13f71ac8bec44044bb6105f83996ed60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "replaying log with TBR, completed traces ::   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66a15a42a9394b61ae22afa1d801f940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "aligning log, completed variants ::   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee4660bcaaf549848752fee9902af1d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "computing precision with alignments, completed variants ::   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bc8f43007684b52a728263e5fb3fc5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "replaying log with TBR, completed traces ::   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68e35f44e6ac494193a7279ecba5b74b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "aligning log, completed variants ::   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c52f89b26d574cd282f9f11772ebadbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "computing precision with alignments, completed variants ::   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b93617c519a248cb947aa5c5f1a23912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "replaying log with TBR, completed traces ::   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caf58a9746cf4c1285f4f00897985c99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "aligning log, completed variants ::   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87ce086391134c41aed870fdae4fe69a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "computing precision with alignments, completed variants ::   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e041eb752fc34fe680c9f373b8158b34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "replaying log with TBR, completed traces ::   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = read_ui_log_as_dataframe(log_path)\n",
    "\n",
    "for exec in executions:\n",
    "    exec_dir = f\"{case_study_name}_{exec['image_weight']}_{exec['text_weight']}\"\n",
    "    exec_path = os.path.join(root_dir, exec_dir)\n",
    "    os.makedirs(exec_path, exist_ok=True)\n",
    "\n",
    "    image_weight = exec['image_weight']\n",
    "    text_weight = exec['text_weight']\n",
    "    \n",
    "    df = extract_features_from_images(df, image_col, text_col, image_weight, text_weight, image_dir)\n",
    "\n",
    "\n",
    "    df, clustering_scores, optimal_clusters, optimal_metrics = cluster_images(df, n_clusters_range, use_pca, n_components)\n",
    "\n",
    "    df = auto_process_id_assignment(df)\n",
    "    caminos_inicial = extraer_caminos(df)\n",
    "    df = eliminar_acciones_duplicadas(df, columna_label='activity_label')\n",
    "\n",
    "    petri_net_process(df, timestamp_col)\n",
    "    bpmn_process(df, timestamp_col)\n",
    "\n",
    "    df.to_csv(os.path.join(exec_path, 'df.csv'), index=False)\n",
    "    move_and_overwrite('results/pn.dot', os.path.join(exec_path, 'pn.dot'))\n",
    "    move_and_overwrite('results/bpmn.dot', os.path.join(exec_path, 'bpmn.dot'))\n",
    "    move_and_overwrite('results/bpmn.bpmn', os.path.join(exec_path, 'bpmn.bpmn'))\n",
    "\n",
    "    caminos_final = extraer_caminos(df)\n",
    "    caminos_inicial_set = set(caminos_inicial)\n",
    "    caminos_final_set = set(caminos_final.apply(tuple))\n",
    "    caminos_apriori_set = set(caminos_apriori_series.apply(tuple))\n",
    "    caminos_nuevos = caminos_final_set - caminos_apriori_set\n",
    "    caminos_no_descubiertos = caminos_apriori_set - caminos_final_set\n",
    "    porcentaje_nuevos = (len(caminos_nuevos) / len(caminos_final_set)) * 100 if caminos_final_set else 0\n",
    "    porcentaje_no_descubiertos = (len(caminos_no_descubiertos) / len(caminos_apriori_set)) * 100 if caminos_apriori_set else 0\n",
    "\n",
    "    with open(os.path.join(exec_path, 'caminos_stats.txt'), 'w') as file:\n",
    "        file.write(f\"Descubrimiento de caminos\\n\")\n",
    "        file.write(f\"Pesos utilizados - Peso de imagen: {image_weight}, Peso de texto: {text_weight}\\n\")  # Nueva línea agregada\n",
    "        file.write(f\"Caminos a priori: {caminos_apriori}\\n\")\n",
    "        file.write(f\"Caminos iniciales: {caminos_inicial_set}\\n\")\n",
    "        file.write(f\"Caminos finales: {caminos_final_set}\\n\")\n",
    "        file.write(f\"Porcentaje de nuevos caminos: {porcentaje_nuevos:.2f}%\\n\")\n",
    "        file.write(f\"Caminos no descubiertos: {porcentaje_no_descubiertos:.2f}%\\n\")\n",
    "\n",
    "\n",
    "    results.append({\n",
    "        'exec': exec['exec'],\n",
    "        'image_weight': image_weight,\n",
    "        'text_weight': text_weight,\n",
    "        'new%': porcentaje_nuevos,\n",
    "        'pathNotDisc%': porcentaje_no_descubiertos,\n",
    "        'Silhouette': optimal_metrics['silhouette_score'],\n",
    "        'Davies-Bouldin': optimal_metrics['davies_bouldin_score'],\n",
    "        'Calinski-Harabasz': optimal_metrics['calinski_harabasz_score'],\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "overwrite_csv(results_df, os.path.join(root_dir, 'resultados.csv'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'clip' #clip or vgg\n",
    "text_weight = 0.5\n",
    "image_weight = 0.5\n",
    "use_pca = False\n",
    "n_clusters_range = (2, 11)\n",
    "n_components = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_ui_log_as_dataframe(log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = extract_features_from_images(df, image_col, text_col, image_weight, text_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, clustering_scores, optimal_cluster, optimal_metrics = cluster_images(df, n_clusters_range, use_pca, n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLO EJECUTAR SI SE QUIERE RECALCULAR LOS CENTROIDES DE REFERENCIA\n",
    "centroides_referencia = calcular_centroides(np.array(df['combined_features']), df['activity_label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "reasignar_clusters_en_df(df, centroides_referencia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clustering_scores(clustering_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = auto_process_id_assignment(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "caminos_inicial = extraer_caminos(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = eliminar_acciones_duplicadas(df, columna_label='activity_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitness, precision, generalization, simplicity = petri_net_process(df)\n",
    "bpmn_process(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[image_col, text_col, 'activity_label', 'process_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(os.path.join('results', 'df.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "caminos_final = extraer_caminos(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "caminos_inicial_set = set(caminos_inicial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "caminos_final_set = set(caminos_final.apply(tuple))\n",
    "caminos_apriori_set = set(caminos_apriori_series.apply(tuple))\n",
    "\n",
    "caminos_nuevos = caminos_final_set - caminos_apriori_set\n",
    "caminos_no_descubiertos = caminos_apriori_set - caminos_final_set\n",
    "\n",
    "porcentaje_nuevos = (len(caminos_nuevos) / len(caminos_final_set)) * 100 if caminos_final_set else 0\n",
    "porcentaje_no_descubiertos = (len(caminos_no_descubiertos) / len(caminos_apriori_set)) * 100 if caminos_apriori_set else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caminos a priori: 4\n",
      "Caminos iniciales: 4\n",
      "Caminos finales: 4\n",
      "Porcentaje de nuevos caminos: 100.00%\n",
      "Caminos no descubiertos: 100.00%\n"
     ]
    }
   ],
   "source": [
    "print('Caminos a priori:', len(caminos_apriori)) # Caminos óptimos de mi proceso\n",
    "print('Caminos iniciales:', len(caminos_inicial_set)) # Caminos obtenidos del primer modelos de datos\n",
    "print('Caminos finales:', len(caminos_final_set)) # Caminos obtenidos después de aplicar post-procesado (eliminar duplicados de actividades)\n",
    "print(f'Porcentaje de nuevos caminos: {porcentaje_nuevos:.2f}%') # Porcentaje de caminos que están presente en el modelo final pero no se encontraban en el modelo a priori\n",
    "print(f'Caminos no descubiertos: {porcentaje_no_descubiertos:.2f}%') # Porcentaje de caminos que estaban en el modelo a priori pero no se encontraron en el modelo final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_with_separator(text, separator='='):\n",
    "    print(f\"{text}\\n{separator * len(text)}\")\n",
    "\n",
    "print_with_separator(\"\\nParámetros de la ejecución\")\n",
    "print(f'Modelo empleado: {model}')\n",
    "print(f'Peso texto: {text_weight}')\n",
    "print(f'Peso imagen: {image_weight}')\n",
    "print(f\"Rango de clústeres: {n_clusters_range}\")\n",
    "print(f\"Usar PCA: {use_pca}\")\n",
    "print(f\"Número de componentes PCA: {n_components}\")\n",
    "\n",
    "print_with_separator(\"\\nCaminos a priori\")\n",
    "print('Caminos a priori:', caminos_apriori)\n",
    "print('Caminos iniciales:', caminos_inicial_set)\n",
    "print('Caminos finales:', caminos_final_set)\n",
    "print(f'Porcentaje de nuevos caminos: {porcentaje_nuevos:.2f}%')\n",
    "print(f'Caminos no descubiertos: {porcentaje_no_descubiertos:.2f}%')\n",
    "\n",
    "print_with_separator(\"\\nMétricas a nivel del proceso (PM4PY)\")\n",
    "print(f\"Fitness: {fitness['averageFitness']}\")\n",
    "print(f\"Precisión: {precision}\")\n",
    "print(f\"Generalización: {generalization}\")\n",
    "print(f\"Simplicidad: {simplicity}\")\n",
    "\n",
    "print_with_separator(\"\\nMétricas de clusterización\")\n",
    "print(f'Clusterización: {optimal_metrics}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "informacion = f\"\"\"\n",
    "Parámetros de la ejecución\n",
    "Modelo empleado: {model}\n",
    "Peso texto: {text_weight}\n",
    "Peso imagen: {image_weight}\n",
    "Rango de clústeres: {n_clusters_range}\n",
    "Usar PCA: {use_pca}\n",
    "Número de componentes PCA: {n_components}\n",
    "\n",
    "Descubrimiento de caminos\n",
    "Caminos a priori: {caminos_apriori}\n",
    "Caminos iniciales: {caminos_inicial_set}\n",
    "Caminos finales: {caminos_final_set}\n",
    "Porcentaje de nuevos caminos: {porcentaje_nuevos:.2f}%\n",
    "Caminos no descubiertos: {porcentaje_no_descubiertos:.2f}%\n",
    "\n",
    "Métricas a nivel del proceso (PM4PY)\n",
    "Fitness: {fitness['averageFitness']}\n",
    "Precisión: {precision}\n",
    "Generalización: {generalization}\n",
    "Simplicidad: {simplicity}\n",
    "\n",
    "Métricas de clusterización\n",
    "Clusterización: {optimal_metrics}\n",
    "\"\"\"\n",
    "\n",
    "# Escribe la información en el archivo\n",
    "with open(\"results/execution.txt\", \"w\") as archivo:\n",
    "    archivo.write(informacion)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
